{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkerynh\u001b[0m (\u001b[33mkerynhan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/wandb/run-20240811_080407-j8t6yuii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kerynhan/v6_x50_fold/runs/j8t6yuii' target=\"_blank\">polt-voting_eff-B7</a></strong> to <a href='https://wandb.ai/kerynhan/v6_x50_fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kerynhan/v6_x50_fold' target=\"_blank\">https://wandb.ai/kerynhan/v6_x50_fold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kerynhan/v6_x50_fold/runs/j8t6yuii' target=\"_blank\">https://wandb.ai/kerynhan/v6_x50_fold/runs/j8t6yuii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "Loss: 0.0600: 100%|██████████| 3925/3925 [11:12<00:00,  5.83it/s]\n",
      "Validating: 100%|██████████| 982/982 [01:18<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.7895, Train Acc: 0.7542, Train F1: 0.7380\n",
      "Epoch 1/50, Val Loss: 0.1047, Val Acc: 0.9646, Val F1: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 3925/3925 [11:07<00:00,  5.88it/s]\n",
      "Validating: 100%|██████████| 982/982 [01:17<00:00, 12.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.1185, Train Acc: 0.9645, Train F1: 0.9629\n",
      "Epoch 2/50, Val Loss: 0.0112, Val Acc: 0.9972, Val F1: 0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0034: 100%|██████████| 3925/3925 [11:06<00:00,  5.88it/s]\n",
      "Validating: 100%|██████████| 982/982 [01:17<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.0507, Train Acc: 0.9869, Train F1: 0.9865\n",
      "Epoch 3/50, Val Loss: 0.0076, Val Acc: 0.9980, Val F1: 0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0046: 100%|██████████| 3925/3925 [11:32<00:00,  5.67it/s]\n",
      "Validating: 100%|██████████| 982/982 [01:55<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.0374, Train Acc: 0.9904, Train F1: 0.9902\n",
      "Epoch 4/50, Val Loss: 0.0050, Val Acc: 0.9989, Val F1: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 3925/3925 [19:31<00:00,  3.35it/s]\n",
      "Validating: 100%|██████████| 982/982 [02:36<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.0374, Train Acc: 0.9908, Train F1: 0.9904\n",
      "Epoch 5/50, Val Loss: 0.0148, Val Acc: 0.9976, Val F1: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0096: 100%|██████████| 3925/3925 [21:39<00:00,  3.02it/s]\n",
      "Validating: 100%|██████████| 982/982 [02:36<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.0359, Train Acc: 0.9913, Train F1: 0.9908\n",
      "Epoch 6/50, Val Loss: 0.0076, Val Acc: 0.9982, Val F1: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 3925/3925 [21:43<00:00,  3.01it/s]\n",
      "Validating: 100%|██████████| 982/982 [02:33<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.0363, Train Acc: 0.9916, Train F1: 0.9911\n",
      "Epoch 7/50, Val Loss: 0.0079, Val Acc: 0.9980, Val F1: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 3925/3925 [21:42<00:00,  3.01it/s]\n",
      "Validating: 100%|██████████| 982/982 [02:32<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.0372, Train Acc: 0.9911, Train F1: 0.9907\n",
      "Epoch 8/50, Val Loss: 0.0119, Val Acc: 0.9980, Val F1: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2842: 100%|██████████| 3925/3925 [21:36<00:00,  3.03it/s]\n",
      "Validating: 100%|██████████| 982/982 [02:33<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.0415, Train Acc: 0.9905, Train F1: 0.9900\n",
      "Epoch 9/50, Val Loss: 0.0218, Val Acc: 0.9969, Val F1: 0.9966\n",
      "Early stopping triggered at epoch 9\n",
      "Fold 5 Macro F1 Score: 0.9966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1051eb7416546b3b20d00e5b3ecde14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_acc</td><td>▁▇███████</td></tr><tr><td>train_f1</td><td>▁▇███████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁████████</td></tr><tr><td>val_f1</td><td>▁████████</td></tr><tr><td>val_loss</td><td>█▁▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>0.99049</td></tr><tr><td>train_f1</td><td>0.98996</td></tr><tr><td>train_loss</td><td>0.04153</td></tr><tr><td>val_acc</td><td>0.99688</td></tr><tr><td>val_f1</td><td>0.99659</td></tr><tr><td>val_loss</td><td>0.02177</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polt-voting_eff-B7</strong> at: <a href='https://wandb.ai/kerynhan/v6_x50_fold/runs/j8t6yuii' target=\"_blank\">https://wandb.ai/kerynhan/v6_x50_fold/runs/j8t6yuii</a><br/> View project at: <a href='https://wandb.ai/kerynhan/v6_x50_fold' target=\"_blank\">https://wandb.ai/kerynhan/v6_x50_fold</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240811_080407-j8t6yuii/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "import wandb\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# WANDB 초기화\n",
    "wandb.login()\n",
    "wandb.init(project='v6_x50_fold', name='polt-voting_eff-B7')\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터 경로 및 설정\n",
    "data_path_train = 'data/augmented_v3_x50/'\n",
    "data_path_test = 'data/test/'\n",
    "img_size = 224\n",
    "LR = 2e-5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 8\n",
    "patience = 5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT_PROB = 0.7\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = Image.open(os.path.join(self.path, name)).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# EfficientNet-B7 모델 정의\n",
    "class CustomEfficientNetB7(nn.Module):\n",
    "    def __init__(self, num_classes=17, dropout_prob=0.7):\n",
    "        super(CustomEfficientNetB7, self).__init__()\n",
    "        self.model = timm.create_model('tf_efficientnet_b7_ns', pretrained=True)   # tf_efficientnet_b7.ns_jft_in1k\n",
    "        num_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, scaler, scheduler=None, clip_value=0.5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in tqdm(loader, desc=\"Validating\"):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# augmentation을 위한 transform 정의\n",
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.CoarseDropout(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드\n",
    "train_df_test = pd.read_csv('data/augment_v3_x50.csv')\n",
    "test_df_test = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 클래스별 가중치 계산\n",
    "class_counts = train_df_test['target'].value_counts().sort_index()\n",
    "total_samples = len(train_df_test)\n",
    "class_weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = np.array(class_weights)\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Stratified K-Fold 설정\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "folds = list(skf.split(train_df_test['ID'], train_df_test['target']))\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "best_val_loss = float('inf')\n",
    "fold_val_metrics = []\n",
    "best_model_paths = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    if fold == 4:  # 5번째 폴드만 학습\n",
    "        print(f\"Fold {fold + 1}\")\n",
    "\n",
    "        trn_dataset = ImageDataset(\n",
    "            train_df_test.iloc[train_idx],\n",
    "            data_path_train,\n",
    "            transform=trn_transform\n",
    "        )\n",
    "\n",
    "        val_dataset = ImageDataset(\n",
    "            train_df_test.iloc[val_idx],\n",
    "            data_path_train,\n",
    "            transform=tst_transform\n",
    "        )\n",
    "\n",
    "        trn_loader = DataLoader(\n",
    "            trn_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # 모델 정의 (EfficientNet-B7으로 변경)\n",
    "        model = CustomEfficientNetB7(num_classes=17, dropout_prob=DROPOUT_PROB).to(device)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        fold_best_val_loss = float('inf')\n",
    "        fold_best_model_path = f'best_model_fold_{fold + 1}_fold_x50_Eff-B7.pth'\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_metrics = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, scaler, scheduler)\n",
    "            val_metrics = validate(val_loader, model, loss_fn, device)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_metrics['train_loss']:.4f}, Train Acc: {train_metrics['train_acc']:.4f}, Train F1: {train_metrics['train_f1']:.4f}\")\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}, Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.4f}, Val F1: {val_metrics['val_f1']:.4f}\")\n",
    "\n",
    "            # WANDB 로그 기록\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['train_loss'],\n",
    "                'train_acc': train_metrics['train_acc'],\n",
    "                'train_f1': train_metrics['train_f1'],\n",
    "                'val_loss': val_metrics['val_loss'],\n",
    "                'val_acc': val_metrics['val_acc'],\n",
    "                'val_f1': val_metrics['val_f1']\n",
    "            })\n",
    "\n",
    "            scheduler.step(val_metrics['val_loss'])\n",
    "\n",
    "            if val_metrics['val_loss'] < fold_best_val_loss:\n",
    "                fold_best_val_loss = val_metrics['val_loss']\n",
    "                torch.save(model.state_dict(), fold_best_model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        best_model_paths.append(fold_best_model_path)\n",
    "        fold_val_metrics.append((fold_best_val_loss, val_metrics['val_f1']))\n",
    "        print(f\"Fold {fold + 1} Macro F1 Score: {val_metrics['val_f1']:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import timm\n",
    "# import torch\n",
    "# import albumentations as A\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch.nn as nn\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# from torch.optim import Adam\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from PIL import Image\n",
    "# import wandb\n",
    "\n",
    "# # 시드 고정\n",
    "# SEED = 42\n",
    "# os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # WANDB 초기화\n",
    "# wandb.login()\n",
    "# wandb.init(project='v6_x50_fold', name='polt-voting_eff-B7')\n",
    "\n",
    "# # 디바이스 설정\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # 데이터 경로 및 설정\n",
    "# data_path_train = 'data/augmented_v3_x50/'\n",
    "# data_path_test = 'data/test/'\n",
    "# img_size = 224\n",
    "# LR = 2e-5\n",
    "# EPOCHS = 50\n",
    "# BATCH_SIZE = 16\n",
    "# num_workers = 8\n",
    "# patience = 5\n",
    "# WEIGHT_DECAY = 1e-4\n",
    "# DROPOUT_PROB = 0.7\n",
    "\n",
    "# # 데이터셋 클래스 정의\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, df, path, transform=None):\n",
    "#         self.df = df.values\n",
    "#         self.path = path\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         name, target = self.df[idx]\n",
    "#         img = Image.open(os.path.join(self.path, name)).convert(\"RGB\")\n",
    "#         img = np.array(img)\n",
    "#         if self.transform:\n",
    "#             img = self.transform(image=img)['image']\n",
    "#         return img, target\n",
    "\n",
    "# # EfficientNet-B7 모델 정의\n",
    "# class CustomEfficientNetB7(nn.Module):\n",
    "#     def __init__(self, num_classes=17, dropout_prob=0.7):\n",
    "#         super(CustomEfficientNetB7, self).__init__()\n",
    "#         self.model = timm.create_model('tf_efficientnet_b7_ns', pretrained=True)   # tf_efficientnet_b7.ns_jft_in1k\n",
    "#         num_features = self.model.classifier.in_features\n",
    "#         self.model.classifier = nn.Sequential(\n",
    "#             nn.BatchNorm1d(num_features),\n",
    "#             nn.Linear(num_features, 1024),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout_prob),\n",
    "#             nn.Linear(1024, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # 학습 함수 정의\n",
    "# def train_one_epoch(loader, model, optimizer, loss_fn, device, scaler, scheduler=None, clip_value=0.5):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     preds_list = []\n",
    "#     targets_list = []\n",
    "\n",
    "#     pbar = tqdm(loader, desc=\"Training\")\n",
    "#     for image, targets in pbar:\n",
    "#         image = image.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             preds = model(image)\n",
    "#             loss = loss_fn(preds, targets)\n",
    "        \n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "#         targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "#         pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "#     if scheduler:\n",
    "#         scheduler.step()\n",
    "\n",
    "#     train_loss /= len(loader)\n",
    "#     train_acc = accuracy_score(targets_list, preds_list)\n",
    "#     train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "#     ret = {\n",
    "#         \"train_loss\": train_loss,\n",
    "#         \"train_acc\": train_acc,\n",
    "#         \"train_f1\": train_f1,\n",
    "#     }\n",
    "\n",
    "#     return ret\n",
    "\n",
    "# # 검증 함수 정의\n",
    "# def validate(loader, model, loss_fn, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     preds_list = []\n",
    "#     targets_list = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for image, targets in tqdm(loader, desc=\"Validating\"):\n",
    "#             image = image.to(device)\n",
    "#             targets = targets.to(device)\n",
    "\n",
    "#             preds = model(image)\n",
    "#             loss = loss_fn(preds, targets)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "#             targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "#     val_loss /= len(loader)\n",
    "#     val_acc = accuracy_score(targets_list, preds_list)\n",
    "#     val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "#     ret = {\n",
    "#         \"val_loss\": val_loss,\n",
    "#         \"val_acc\": val_acc,\n",
    "#         \"val_f1\": val_f1,\n",
    "#     }\n",
    "\n",
    "#     return ret\n",
    "\n",
    "# # augmentation을 위한 transform 정의\n",
    "# trn_transform = A.Compose([\n",
    "#     A.Resize(height=img_size, width=img_size),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "#     A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "#     A.CoarseDropout(p=0.5),\n",
    "#     A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# tst_transform = A.Compose([\n",
    "#     A.Resize(height=img_size, width=img_size),\n",
    "#     A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# # 데이터 로드\n",
    "# train_df_test = pd.read_csv('data/augment_v3_x50.csv')\n",
    "# test_df_test = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# # 클래스별 가중치 계산\n",
    "# class_counts = train_df_test['target'].value_counts().sort_index()\n",
    "# total_samples = len(train_df_test)\n",
    "# class_weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "# class_weights = np.array(class_weights)\n",
    "# class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# # Stratified K-Fold 설정\n",
    "# n_splits = 5\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "# folds = list(skf.split(train_df_test['ID'], train_df_test['target']))\n",
    "\n",
    "# # 학습 및 검증 루프\n",
    "# best_val_loss = float('inf')\n",
    "# fold_val_metrics = []\n",
    "# best_model_paths = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "#     print(f\"Fold {fold + 1}\")\n",
    "\n",
    "#     trn_dataset = ImageDataset(\n",
    "#         train_df_test.iloc[train_idx],\n",
    "#         data_path_train,\n",
    "#         transform=trn_transform\n",
    "#     )\n",
    "\n",
    "#     val_dataset = ImageDataset(\n",
    "#         train_df_test.iloc[val_idx],\n",
    "#         data_path_train,\n",
    "#         transform=tst_transform\n",
    "#     )\n",
    "\n",
    "#     trn_loader = DataLoader(\n",
    "#         trn_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=True,\n",
    "#         drop_last=False\n",
    "#     )\n",
    "\n",
    "#     val_loader = DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     # 모델 정의 (EfficientNet-B7으로 변경)\n",
    "#     model = CustomEfficientNetB7(num_classes=17, dropout_prob=DROPOUT_PROB).to(device)\n",
    "\n",
    "#     loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "#     scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "#     scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#     fold_best_val_loss = float('inf')\n",
    "#     fold_best_model_path = f'best_model_fold_{fold + 1}_fold_x50_Eff-B7.pth'\n",
    "#     patience_counter = 0\n",
    "\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         train_metrics = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, scaler, scheduler)\n",
    "#         val_metrics = validate(val_loader, model, loss_fn, device)\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_metrics['train_loss']:.4f}, Train Acc: {train_metrics['train_acc']:.4f}, Train F1: {train_metrics['train_f1']:.4f}\")\n",
    "#         print(f\"Epoch {epoch + 1}/{EPOCHS}, Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.4f}, Val F1: {val_metrics['val_f1']:.4f}\")\n",
    "\n",
    "#         # WANDB 로그 기록\n",
    "#         wandb.log({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'train_loss': train_metrics['train_loss'],\n",
    "#             'train_acc': train_metrics['train_acc'],\n",
    "#             'train_f1': train_metrics['train_f1'],\n",
    "#             'val_loss': val_metrics['val_loss'],\n",
    "#             'val_acc': val_metrics['val_acc'],\n",
    "#             'val_f1': val_metrics['val_f1']\n",
    "#         })\n",
    "\n",
    "#         scheduler.step(val_metrics['val_loss'])\n",
    "\n",
    "#         if val_metrics['val_loss'] < fold_best_val_loss:\n",
    "#             fold_best_val_loss = val_metrics['val_loss']\n",
    "#             torch.save(model.state_dict(), fold_best_model_path)\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "#             break\n",
    "\n",
    "#     best_model_paths.append(fold_best_model_path)\n",
    "#     fold_val_metrics.append((fold_best_val_loss, val_metrics['val_f1']))\n",
    "#     print(f\"Fold {fold + 1} Macro F1 Score: {val_metrics['val_f1']:.4f}\")\n",
    "\n",
    "# wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Validation Loss: 0.0036, Validation Accuracy: 0.9992, Validation F1 Score: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Validation Loss: 0.0049, Validation Accuracy: 0.9987, Validation F1 Score: 0.9987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Validation Loss: 0.0056, Validation Accuracy: 0.9989, Validation F1 Score: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Validation Loss: 0.0046, Validation Accuracy: 0.9988, Validation F1 Score: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Validation Loss: 0.0051, Validation Accuracy: 0.9989, Validation F1 Score: 0.9989\n",
      "Mean Validation Loss: 0.0047, Mean Validation Accuracy: 0.9989, Mean Validation F1 Score: 0.9989\n",
      "최종 제출 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측 및 결과 저장\n",
    "test_dataset = ImageDataset(test_df_test, data_path_test, transform=tst_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model_paths = [\n",
    "    'best_model_fold_1_fold_x50_Eff-B7.pth',\n",
    "    'best_model_fold_2_fold_x50_Eff-B7.pth',\n",
    "    'best_model_fold_3_fold_x50_Eff-B7.pth',\n",
    "    'best_model_fold_4_fold_x50_Eff-B7.pth',\n",
    "    'best_model_fold_5_fold_x50_Eff-B7.pth'\n",
    "]\n",
    "\n",
    "fold_preds = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "val_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    model = CustomEfficientNetB7(num_classes=17, dropout_prob=0.7).to(device)\n",
    "    model.load_state_dict(torch.load(model_paths[fold]))\n",
    "    model.eval()\n",
    "\n",
    "    val_dataset = ImageDataset(train_df_test.iloc[val_idx], data_path_train, transform=tst_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    val_loss = 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "    val_f1_scores.append(val_f1)\n",
    "    val_acc_scores.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            preds.append(output.softmax(dim=1).cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    fold_preds.append(preds)\n",
    "\n",
    "# 폴드별 예측을 평균하여 최종 예측 생성\n",
    "final_preds = np.mean(fold_preds, axis=0)\n",
    "final_class_preds = np.argmax(final_preds, axis=1)\n",
    "\n",
    "# 결과 저장\n",
    "submission = pd.DataFrame({'ID': test_df_test['ID'], 'target': final_class_preds})\n",
    "submission.to_csv('submit_v6_x50_fold_Eff-B8.csv', index=False)\n",
    "\n",
    "# 최종 검증 성능 출력\n",
    "print(f\"Mean Validation Loss: {np.mean(val_losses):.4f}, Mean Validation Accuracy: {np.mean(val_acc_scores):.4f}, Mean Validation F1 Score: {np.mean(val_f1_scores):.4f}\")\n",
    "print(\"최종 제출 파일이 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
